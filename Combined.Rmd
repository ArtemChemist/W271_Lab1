
---
title: "Lab 1, Short Questions"
geometry: margin=1in
output:
  pdf_document:
    toc: yes
    number_sections: yes
fontsize: 11pt
---

```{r load packages for homework 2, message=FALSE, warning=FALSE, echo = FALSE}
library(tidyverse)
library(patchwork)
library(nnet)
library(car)
library(ggpubr)
library(stargazer)
library(GGally)
library(nnet)
library(car)
library(corrplot)
library(gridExtra)
```

# Strategic Placement of Products in Grocery Stores (5 points)

These questions are taken from Question 12 of chapter 3 of the textbook(Bilder and Loughin’s “Analysis of Categorical Data with R.

> *In order to maximize sales, items within grocery stores are strategically placed to draw customer attention. This exercise examines one type of item—breakfast cereal. Typically, in large grocery stores, boxes of cereal are placed on sets of shelves located on one side of the aisle. By placing particular boxes of cereals on specific shelves, grocery stores may better attract customers to them. To investigate this further, a random sample of size 10 was taken from each of four shelves at a Dillons grocery store in Manhattan, KS. These data are given in the *cereal_dillons.csv *file. The response variable is the shelf number, which is numbered from bottom (1) to top (4), and the explanatory variables are the sugar, fat, and sodium content of the cereals.*

```{r read cereal data, message=FALSE}
cereal <- read_csv("../data/short-questions/cereal_dillons.csv")

# Rename raw data columns to keep final names short
names(cereal) <- c("ID", "Shelf", "Cereal", "size_g", 
                   "sugar_raw", "fat_raw", "sodium_raw")
```


## Recode Data 
(1 point) The explanatory variables need to be reformatted before proceeding further (sample code is provided in the textbook). First, divide each explanatory variable by its serving size to account for the different serving sizes among the cereals. Second, rescale each variable to be within 0 and 1. Construct side-by-side box plots with dot plots overlaid for each of the explanatory variables. Also, construct a parallel coordinates plot for the explanatory variables and the shelf number. Discuss whether possible content differences exist among the shelves.

```{r recode data}
# Re-scale columns in the following list
# Create new columns for re-scaled data
col_trans_list <- c("fat_raw", "sugar_raw", "sodium_raw") 

for (col in col_trans_list) {
  col_per <- cereal[col] / cereal["size_g"]
  cereal[str_sub(col, 1, -5)] <- (col_per - min(col_per)) / max(col_per)
}

cereal
```

```{r boxplots}
par(mfrow = c(1, 3))

# Sugar
boxplot(formula = sugar ~ Shelf, data = cereal, ylab = "Sugar",
        xlab = "Shelf", pars = list(outpch = NA))
stripchart(x = cereal$sugar ~ cereal$Shelf, lwd = 2,
           col = "red", method = "jitter", vertical = TRUE, pch = 1, add = TRUE)

# Fat
boxplot(formula = fat ~ Shelf, data = cereal, ylab = "Fat",
        xlab = "Shelf", pars = list(outpch = NA))
stripchart(x = cereal$fat ~ cereal$Shelf, lwd = 2,
           col = "red", method = "jitter", vertical = TRUE, pch = 1, add = TRUE)

# Sodium
boxplot(formula = sodium ~ Shelf, data = cereal, ylab = "Sodium",
        xlab = "Shelf", pars = list(outpch = NA))
stripchart(x = cereal$sodium ~ cereal$Shelf, lwd = 2,
           col = "red", method = "jitter", vertical = TRUE, pch = 1, add = TRUE)
```

> It appears that the items that are higher in sodium tend to be on shelf #1. Items that are high in sugar tend to go on shelf #2. For fat, there does not appear to be any obvious trends of which shelf items that are high or low in fat go to.

```{r parallel coordiantes plot}
cereal$ShelfNumber <- as.character(cereal$Shelf)
ggparcoord(data = cereal, columns = 8:10,groupColumn = "ShelfNumber",
           showPoints = TRUE, scale = "uniminmax")
```

> The parallel coordinates plot is harder to glean information from than the box plots. When looking closely one can find much of the same information that was found in the box plots. Namely, the highest sugar items belonging to shelf #2 and the high sodium items being found in shelf #1.

> Without running a regression analysis it is not responsible to say definitively whether or not a content different exists between the shelves. Based on these graphs there doesn't seem to be too big of a relationship between sugar, fat, and sodium and the shelf a cereal belongs to.

## Evaluate Ordinal vs. Categorical 
(1 point) The response has values of $1, 2, 3,$ and $4$. Explain under what setting would it be desirable to take into account ordinality, and whether you think that this setting occurs here. Then estimate a suitable multinomial regression model with linear forms of the sugar, fat, and sodium variables. Perform LRTs to examine the importance of each explanatory variable. Show that there are no significant interactions among the explanatory variables (including an interaction among all three variables).

> Ordinal data would make sense in the case where there is a natural ordering to the shelves. Such as if higher shelves were inherently more desirable than lower shelves. Then it would be expected that better selling products be placed on higher shelves. It does not appear as if this is the case here as the most desirable shelves would likely be at eye-level (towards the middle) and there doesn't appear to be any other sort of ordering at play.

```{r multinomial logit regressions, message=FALSE, results = FALSE}
# Set Shelf as a categorical value
cereal$Shelf <- factor(cereal$Shelf, levels = c("1", "2", "3", "4"))

# Estimate linear model
model_cereal_shelves_linear <- multinom(
  formula = Shelf ~ fat + sugar + sodium,
  data = cereal
)

# Estimate linear model with all interactions
model_cereal_shelves_quadratic <- multinom(
  formula = Shelf ~ fat + sugar + sodium +
    fat:sugar + fat:sodium + sodium:sugar +
    sodium:sugar:fat,
  data = cereal
)
```

```{r likelihood ratio tests: main effects}
# Conduct Anova test on linear model
lrt_cereal_main_effects <- car::Anova(model_cereal_shelves_linear, test = "LR")
lrt_cereal_main_effects
```

```{r likelihood ratio tests: interaction effects} 
# Conduct Anova test on the interactions
lrt_cereal_quadratic_effects <- car::Anova(model_cereal_shelves_quadratic, test = "LR")
lrt_cereal_quadratic_effects
```
> The LRT test for linear response variables shows strong statistical significance for sugar and sodium content (p-values of `r lrt_cereal_main_effects$"Pr(>Chisq)"[2]` and `r lrt_cereal_main_effects$"Pr(>Chisq)"[3]` respectievely) but fails to show even weak significance of fat content (p-value of `r round(lrt_cereal_main_effects$"Pr(>Chisq)"[1], digits = 4)`)

> The LRT test for the interactions further revealed that no two-way or three way interaction achieved statistical significance

> Similar to what the graphs show there is clearly an obvious relationship between sugar, sodium, and shelf placement. The Anova test shows that none of the interaction terms are significant as well as the linear fat variable.

## Where do you think Apple Jacks will be placed? 
(1 point) Kellogg’s Apple Jacks (http://www.applejacks.com) is a cereal marketed toward children. For a serving size of $28$ grams, its sugar content is $12$ grams, fat content is $0.5$ grams, and sodium content is $130$ milligrams. Estimate the shelf probabilities for Apple Jacks.

```{r predictions for apple jacks, message=FALSE, results = FALSE}
# Estimate new model that removes non-significant fat variable
model_cereal_shelves_trim <- multinom(formula = Shelf ~ sugar + sodium,
                                      data = cereal)

# Create a datframe with Apple Jack data
app_jack <- data.frame(size_g = 28, 
                       sugar_raw = 12, 
                       fat_raw = 0.5, sodium_raw = 130)

# Use the same normalization procedures as for the main dataframe
for (col in col_trans_list) {
  # Column of variable devided by portion size:
  col_per <- app_jack[col] / app_jack["size_g"] 
  
  # Reference column of variable devided by portion size:
  ref_col <- cereal[col] / cereal["size_g"]
  app_jack[str_sub(col, 1, -5)] <- (col_per - min(ref_col)) / max(ref_col)
}

# Estimate placement of Apple Jack with a trimmed model
aj_shelf_probs_trim <- predict(model_cereal_shelves_trim, 
                               newdata = app_jack, type = "probs")
shelf_trim <- aj_shelf_probs_trim[which.max(aj_shelf_probs_trim)]

# Estimate placement of Apple Jack with bloated model
aj_shelf_probs <- predict(model_cereal_shelves_linear, 
                          newdata = app_jack, type = "probs")
shelf <- aj_shelf_probs[which.max(aj_shelf_probs)]

aj_shelf_probs
```

> Using best practices for variable selection we estimated a new model that only contains statistically significant variables: sugar and sodium. Using this model we estimate probabilities of placing Apple Jack on the shelves 1, 2, 3 and 4 respectively as `r round(100* aj_shelf_probs_trim, digits = 0)` percentage points. Thus, shelf `r which.max(aj_shelf_probs)` is clearly the most likely place. If we were to use a full model, that includes insignificant variable Fat, the result would stay the same, but the difference between shelves would be smaller at `r round(100* aj_shelf_probs, digits = 0)` percentage points.

## Figure 3.3 
(1 point) Construct a plot similar to Figure 3.3 where the estimated probability for a shelf is on the *y-axis* and the sugar content is on the *x-axis*. Use the mean overall fat and sodium content as the corresponding variable values in the model. Interpret the plot with respect to sugar content.

```{r create figure 3.3, fig.width=6, fig.height=3}
# Get mean values for static values
fat_mean <- mean(cereal$fat)
na_mean <- mean(cereal$sodium)

# Make dataframe with values used fo inference
df_to_plot <- data.frame(fat = rep(fat_mean, times = 100), 
                         sodium = rep(na_mean, times = 100), 
                         sugar = seq(1, 100) / 100)

# Attach predicted values to this dataframe
df_to_plot <- cbind(df_to_plot, 
    predict(model_cereal_shelves_linear, newdata = df_to_plot, type = "probs"))

# Plot the data 
shelf_vs_sugar_plot <- df_to_plot %>%
  pivot_longer(cols = c(4:7), names_to = "indicators", values_to = "values") %>%
  ggplot(data = ., aes(x = sugar, y = values, colour = indicators)) +
  geom_line() +
  theme_minimal() + 
  theme(legend.position = c(0.1, 0.8)) + guides(color=guide_legend("Shelf")) + 
  theme(axis.title.y = element_blank())

shelf_vs_sugar_plot
```

> For cereals with normalized sugar content up to approximately average, there is roughly eqaul chance of finding them on 4'th and 3'd shelfs. Assuming the first shelf is at teh bottom and the 4th is at the top,  an average health-concious adult might see them there. As the normalized sugar content approaches the higher end of the spectrum, the chances of finding this cereal on the second shelf, where a kid might see it, is growing dramatically.

## Odds ratios 
(1 point) Estimate odds ratios and calculate corresponding confidence intervals for each explanatory variable. Relate your interpretations back to the plots constructed for this exercise. 

```{r produce odds ratios}
coefs.2 <- coef(model_cereal_shelves_linear)[1,]
se.2 <- round(summary(model_cereal_shelves_linear)$standard.errors[1,], 2)
ci.2 <- round(data.frame(estimate = exp(coefs.2),
                         lower = exp(coefs.2 - 1.96*se.2),
                         upper =  exp(coefs.2 + 1.96*se.2)), 2)
ci.2

coefs.3 <- coef(model_cereal_shelves_linear)[2,]
se.3 <- round(summary(model_cereal_shelves_linear)$standard.errors[2,], 2)
ci.3 <- round(data.frame(estimate = exp(coefs.3),
                         lower = exp(coefs.3 - 1.96*se.3),
                         upper =  exp(coefs.3 + 1.96*se.3)), 2)
ci.3

coefs.4 <- coef(model_cereal_shelves_linear)[3,]
se.4 <- round(summary(model_cereal_shelves_linear)$standard.errors[3,], 2)
ci.4 <- round(data.frame(estimate = exp(coefs.4),
                         lower = exp(coefs.4 - 1.96*se.4),
                         upper =  exp(coefs.4 + 1.96*se.4)), 2)
ci.4

odds_ratios <- c(ci.2, ci.3, ci.4)
```

> One can see that the estimate and corresponding interval for shelf 2's parameters indicate that the odds increase substantially as `fat` and `sugar` increase. It is also seen that the odds increase very slowly for shelves 2-4 as `sodium` increases showing that shelf 1 is the most probable shelf when sodium is high. Lastly, the confidence intervals are fairly wide which is a result of the weak correlation between the variables and shelf number as well as the limited number of samples.


# Alcohol, self-esteem and negative relationship interactions (5 points)

Read the example **'Alcohol Consumption'** in chapter 4.2.2 of the textbook(Bilder and Loughin’s “Analysis of Categorical Data with R). This is based on a study in which moderate-to-heavy drinkers (defined as at least 12 alcoholic drinks/week for women, 15 for men) were recruited to keep a daily record of each drink that they consumed over a 30-day study period. Participants also completed a variety of rating scales covering daily events in their lives and items related to self-esteem. The data are given in the *DeHartSimplified.csv *data set. Questions 24-26 of chapter 3 of the textbook also relate to this data set and give definitions of its variables: the number of drinks consumed (`numall`), positive romantic-relationship events (`prel`), negative romantic-relationship events (`nrel`), age (`age`), trait (long-term) self-esteem (`rosn`), state (short-term) self-esteem (`state`).

The researchers stated the following hypothesis:

> *We hypothesized that negative interactions with romantic partners would be associated with alcohol consumption (and an increased desire to drink). We predicted that people with low trait self-esteem would drink more on days they experienced more negative relationship interactions compared with days during which they experienced fewer negative relationship interactions. The relation between drinking and negative relationship interactions should not be evident for individuals with high trait self-esteem.*

```{r read drinking data, message=FALSE}
drinks <- read_csv('../data/short-questions/DeHartSimplified.csv')
drinks.selected <- select(drinks, "numall", "prel", "nrel",
                          "age", "rosn", "posevent", "negevent", "desired")
saturday <- filter(drinks, dayweek == "6")
```

## EDA 
(2 points) Conduct a thorough EDA of the data set, giving special attention to the relationships relevant to the researchers' hypotheses. Address the reasons for limiting the study to observations from only one day.

```{r drinking EDA 1, warning=FALSE, fig.width=10, fig.height=4}
drinks_scaled <- as.data.frame(scale(drinks))
drinks_scaled %>%
  pivot_longer(cols = 3:13, names_to = "indicators", values_to = "values") %>%
  ggplot(data = ., aes(x = indicators, y = values)) +
  geom_boxplot() +
  theme_minimal() +
  theme(axis.title.y = element_blank(), axis.title.x = element_blank())
```

```{r Drinking EDA 2, warning=FALSE}
M = cor(drinks.selected,  use = "pairwise.complete.obs")
corrplot(M, method = 'number')
```

```{r Drinking EDA 3}
summary(drinks)
```
```{r Drinking EDA 4, warning=FALSE}
output_variable <- "numall"
input_variables <- setdiff(names(drinks.selected), output_variable)

plots <- lapply(input_variables, function(variable) {
  ggplot(drinks.selected, aes_string(x = variable, y = output_variable)) +
    geom_point() +
    labs(x = variable, y = output_variable) +
    ggtitle(paste(variable, "vs", output_variable))
})

gridExtra::grid.arrange(grobs = plots, ncol = 4)
```

> When visually inspecting each of the covariances and scatterplots it appears that there may be a relationship between some of the variables and the number of drinks but it appears as most of them will not be strong indicators of the final number of drinks consumed. `desired` has the strongest relationship with `numall` with a covariance of `0.40`. For some of the variables the pattern seems to defy expectation such as with `rosn` where those with the strongest sense of self-confidence appeared to drink more.

## Hypothesis One 
(2 points) The researchers hypothesize that negative interactions with romantic partners would be associated with alcohol consumption and an increased desire to drink. Using appropriate models, evaluate the evidence that negative relationship interactions are associated with higher alcohol consumption and an increased desire to drink.

```{r H1, warning=FALSE, fig.width=10, fig.height=4}
deviance_plot <- function(model) {
  s.res <- rstandard(model, type = "pearson")
  lin.pred <- model$linear.predictors
  df <- data.frame(s.res, lin.pred)
  df %>%
    ggplot(aes(x = df$lin.pred , y = df$s.res)) +
    geom_point() +
    geom_hline(yintercept=c(3, 2, 0, -2, -3), color = "red", linetype = "dashed") +
    geom_smooth(se = FALSE) +
    ggtitle("Standardized residuals") + 
    xlab("Linear predictor") +
    ylab("Standardized Pearson residuals")
}

# Poisson
model_nrel_saturday <- glm(formula = numall ~ nrel,
  family = poisson,
  data = saturday
)

summary(model_nrel_saturday)

# Quasi-Poisson
model_quasi_nrel_saturday <- glm(formula = numall ~ nrel,
  family = quasipoisson(link = "log"),
  data = saturday
)

summary(model_quasi_nrel_saturday)

# Desired
model_des_saturday <- glm(formula = desired ~ nrel,
  data = saturday
)

summary(model_des_saturday)
deviance_plot(model_nrel_saturday) +
  deviance_plot(model_quasi_nrel_saturday) +
  deviance_plot(model_des_saturday)
```
> By itself, it does `nrel` does not have a significant effect on the number of drinks one has. There does appear to be some possible issues with overdispersion according to the residual plot and the high deviance, so a second model was fitted using a quasi-poisson model. This one did not appear to have issues with overdispersion but also did not indicate a significant relationship between `nrel` and `numall`. A model for `desired` also did not indicate any significant relationship between `nrel` and the desire one has for a drink. Altogether, it does not appear as though one can confirm the hypothesis that `nrel` has an effect on `desired` or `numall`.

## Hypothesis Two
(1 point) The researchers hypothesize that the relation between drinking and negative relationship interactions should not be evident for individuals with high trait self-esteem. Conduct an analysis to address this hypothesis.

```{r H2, warning=FALSE, fig.width=6, fig.height=3}
model_nrel_rosn_saturday <- glm(formula = numall ~ nrel*rosn,
  family = poisson,
  data = saturday
)

summary(model_nrel_rosn_saturday)
deviance_plot(model_nrel_rosn_saturday)
```

> As the researchers predicted it appears that both `nrel` and `nrel:rosn` have a significant effect on `numall`. While the expected number of drinks increases with `nrel`, as `rosn` increases, the effect of `nrel` is dampened. Although, what is considered "high self-esteem" is never specified, it is possible under this model to have a high enough self-esteem that negative relationship interactions no longer have an evident effect on the number of drinks consumed. This confirms the hypothesis the researchers were trying to prove. Additionally, it appears as though adding `rosn` to the variables explains the data well enough that the model does not seem at risk of overdispersion as the previous one was.



---
title: "W271 Group Lab 1"
subtitle: "Investigating the 1986 Space Shuttle Challenger Accident"
author: "Please fill in with your names."
output: bookdown::pdf_document2
fontsize: 11pt
geometry: margin=1in
---

```{=tex}
\begin{abstract} 
The goal of this report is to estimate the probability of an O-ring failure in one of the booster rockets of a space shuttle. This report builds upon the data and the approach presented by Dalal et al. (JASA 1989, 84(408), 945). Using historic data on the O-ring falures, joint temperature and leak-check-pressure we estimate logistoc regression for probability of failure. We then use this model for parametric bootstrap estimation of failure probability and confidence interval. Our findings are very close to the conclusion of the original paper.
\end{abstract}
```
# Introduction

## Research question

```{r load packages, message=FALSE, echo = FALSE,results=FALSE,warning = FALSE}
library(car)
library(sandwich)
library(lmtest)
library(knitr)
library(Hmisc)
library(gridExtra)
library(stargazer)
library(mcprofile)
library(readr)
library(dplyr)
library(finalfit)
library(tidyverse)
library(patchwork)
library(MASS)
```

# Data (20 points)

```{r, echo = FALSE,results=FALSE}
data <- read.csv("../data/raw/challenger.csv")
```

```{r, echo = FALSE,results=FALSE}
summary(data)
```

```{r, echo = FALSE,results=FALSE}
describe(data)
```

```{r, fig.width=8, fig.height=3, echo=FALSE}
# Histograms
par(mfrow=c(1,3))
hist(data$Temp, main = "Joint Temperature Distribution", xlab = "Temp")
hist(data$Pressure, main = "Field Leak-Pressure Distribution", xlab = "Pressure", ylab = "")
hist(as.numeric(data$O.ring), main = "O-ring Issue Distribution", xlab = "O-ring issue", ylab = "")
```

```{r, fig.width=4, fig.height=4, echo=FALSE}
# Define colors based on O.ring values
color <- ifelse(data$O.ring > 0, "red", "dark green")

# Create the scatter plot
plot(data$Temp, data$O.ring, xlab = "Temperature (F)", ylab = "O-ring", main = "O-ring Incidents vs. Temperature", pch = 20, col = color)
# Create a legend
legend("topright", legend = c("Flights with no incidents", "Flight with Incidents"), col = c("dark green", "red"), pch = 20)
```

```{r,fig.width=4,fig.height=2,echo=FALSE,results=FALSE}
data %>%
  count(O.ring) %>%
  mutate(prop = round(prop.table(n), 2)) %>%
  kable(col.names = c("Incidents", "Number", "Proportion"))
```

```{r, fig.width=12,fig.height=6,echo=FALSE}
data <- data %>%
  mutate(failure = if_else(data$O.ring == 1 | data$O.ring == 2, "Yes", "No"))

p5 <- data %>%
  ggplot(aes(failure, Temp)) +
  geom_boxplot(aes(fill = failure)) +
  coord_flip() +
  ggtitle("O-ring Incident by Temperature") +
  theme(plot.title = element_text(lineheight = 1, face = "bold")) +
  ylab("Failure") +
  xlab("Temperature")

p6 <- data %>%
  ggplot(aes(failure, Pressure)) +
  geom_boxplot(aes(fill = failure)) +
  coord_flip() +
  ggtitle("O-ring Incident by Pressure") +
  theme(plot.title = element_text(lineheight = 1, face = "bold")) +
  ylab("Failure") +
  xlab("Pressure")

plot_box <- function(data, x, y, title) {
  ggplot(data, aes(factor(x), y)) +
    geom_boxplot(aes(fill = factor(x))) +
    geom_jitter() +
    coord_flip() +
    ggtitle(title) +
    theme(plot.title = element_text(lineheight = 1, face = "bold"))
}

p3 <- plot_box(data,
  x = data$O.ring, y = data$Temp,
  title = "O-ring Incident by Temperature"
)

p4 <- plot_box(data,
  x = data$O.ring, y = data$Pressure,
  title = "O-ring Incident by Pressure"
)

grid.arrange(p5, p6, p3, p4, nrow = 4, ncol = 1)
```

From Data EDA, we don't see any anomalous observations. The launch temperature is approximately normal distributed with a mean of 65-70F. In the sample, approximately 70% of the launches had zero O-ring incidents. Preliminary graphs show that temperature has an effect on O-ring incidents. Lower temperature might increase the probability of O-ring incidents.

## Description

The data used in this analysis pertains to the Challenger Space Shuttle disaster and was generated by recording observations of major parameters such as temperature, pressure and O-ring incidents across different shuttle launches. It is sampled from a population of all shuttle launches. The random variables are temperature, pressure, and O-ring failure status. The specific values of these random variables are produced from the actual launches. The sampling process is not a random sampling process because it doesn't involve randomly selecting elements from a larger population. Rather, it involves recording observations from a sequence of events (the shuttle launches) that actually took place. Each launch can be thought of as an experiment where the temperature and pressure are somewhat uncontrollable variables and the O-ring failure status is the outcome.

The assumption of independence in the context of logistic regression means that we assume the outcome (O-ring failure) of one launch doesn't affect the outcome of another launch. This is necessary because if the outcomes were dependent, then conditional probability would need to be accounted for. The dependence of one outcome on another would violate the independence assumption of the logistic regression model. As a result, the estimation of the parameters from the usual logistic regression model would not have statistical significance. The potential problem with this assumption in the current context is that the launches are not truly independent events. Factors like technological advancements, changes in maintenance practices, and learning from past failures could cause later launches to be dependent on earlier ones. Due to technological advancement, more recent launches might see fewer O-ring incidents, skewing the proportion of incidents lower. On the other hand, the physical conditions of the O-rings might not be independent, for example, if the same set of O-rings were used in multiple launches. The metal fatigue as a result of repeated usage of the same set of O-rings could skew the incident proportion higher.

## Key Features

Data Source and Context: The data set is derived from the Challenger Space Shuttle missions, detailing the conditions of each launch, particularly focusing on temperature, pressure, and the crucial aspect of O-ring failure. The observations represent specific shuttle launches under various conditions.

Variables: The primary variables in this data set include temperature (Temp), pressure (Pressure), and the status of the O-ring (O.ring). Each variable provides critical information about the conditions during each space shuttle launch.

Target Variable: The target variable in the analysis is O-ring failure, a binary variable that indicates whether an O-ring failure occurred during a launch (1,2 = failure, 0 = no failure).

Explanatory Variables: The temperature and pressure during each launch are the explanatory variables. These continuous variables provide context for the conditions that may influence O-ring failure.

Data Structure: The data set is structured as a cross-sectional data set, with each row representing a different space shuttle launch. Random variables "temperature" and "pressure" are column variables "Temp" and "Pressure", respectively.

# Analysis

Temperature (Temp):

Distribution: The temperature during the launches varied from 53°F to 81°F. The distribution of temperature values was fairly normal but slightly skewed towards the higher end.

Relationship with O-ring failure: The exploratory data analysis and logistic regression model indicated a significant relationship between temperature and O-ring failure. Specifically, lower temperatures were associated with a higher risk of O-ring failure. This relationship was found to be statistically significant, making temperature a key variable to consider when predicting O-ring failure.

Insights: The fact that colder temperatures increase the risk of O-ring failure can have important practical implications. For example, it could inform decisions about the optimal conditions for launching a space shuttle to minimize the risk of O-ring failure.

Pressure (Pressure):

Distribution: The pressure during the launches was mostly around 200 psi, with a few instances of lower pressure at 50 psi and 100 psi. The distribution was therefore skewed towards the higher end.

Relationship with O-ring failure: The exploratory data analysis and logistic regression model did not indicate a statistically significant relationship between pressure and O-ring failure. Although pressure was initially included as an explanatory variable in the model, the likelihood ratio test suggested that it did not significantly improve the model's fit to the data.

Insights: While it might seem logical that higher pressure could increase the risk of O-ring failure, the data did not provide statistical evidence for this. Therefore, based on this data set, pressure does not appear to be a key variable to consider when predicting O-ring failure.

In conclusion, while both temperature and pressure are important factors in the overall operating conditions during a space shuttle launch, temperature appears to be the more crucial variable when it comes to predicting O-ring failure.

## Reproducing Previous Analysis (10 points)

```{r, results=FALSE, results='asis'}
data$O.ring <- as.factor(data$O.ring)

# Fit the logistic regression model
model <- glm(data$O.ring ~ Temp + Pressure, data = data, family = binomial())

# Fit reduced models
model_temp <- glm(data$O.ring ~ Temp, data = data, family = binomial())
model_pressure <- glm(data$O.ring ~ Pressure, data = data, family = binomial())

stargazer(model, 
  type = 'latex',
  header=FALSE,
  model.numbers = FALSE,
  digits=2,
  notes.append = FALSE
  )
```

The coefficient of -0.228671 indicates that for each additional degree in temperature, the log-odds of O-ring failure decrease by about 0.23, holding pressure constant. This would suggest that higher temperatures are associated with a lower risk of O-ring failure. Because the p-value (0.0376) is below 0.05, this result is statistically significant at the 5% level.

The coefficient of 0.010400 suggests that for each additional unit increase in pressure, the log-odds of O-ring failure increase by about 0.01, holding temperature constant. However, the p-value (0.2468) is greater than 0.05, suggesting that this result is not statistically significant at the 5% level. Therefore, based on this model, pressure does not have a significant effect on the log-odds of O-ring failure.

From a practical perspective, temperature seems to have a significant effect on O-ring failure, with higher temperatures reducing the likelihood of failure. However, pressure doesn't seem to significantly affect the likelihood of failure.

```{r}
# Perform likelihood ratio tests
anova(model_temp, model, test = "LRT")
```

The test compares the deviances of the two models to assess whether adding Pressure to the model significantly improves the model fit. The deviance difference (1.5331) is not statistically significant (p=0.2156), as indicated by the Pr(\>Chi) value. This means that, based on this test, adding Pressure as a predictor does not significantly improve the model.

```{r}
anova(model_pressure, model, test = "LRT")
```

The test compares the deviances of the two models to assess whether adding Temp to the model significantly improves the model fit. The deviance difference (7.7542) is statistically significant (p=0.005359), as indicated by the Pr(\>Chi) value. This means that adding Temp as a predictor significantly improves the model.

In summary, these results suggest that Temp is a significant predictor of O.ring failure, while Pressure is not, at least not when added to a model that already includes Temp.

```{r}
exp(coef(model))
```

The exponentiated coefficient for Temp is 0.7955903. This means that for each unit increase in Temp, the odds of O-ring failure are multiplied by approximately 0.80, or in other words, they decrease by about 20%, assuming all other variables are held constant. This matches with our previous interpretation that an increase in temperature decreases the likelihood of O-ring failure.

The exponentiated coefficient for Pressure is 1.010454. This means that for each unit increase in Pressure, the odds of O-ring failure are multiplied by approximately 1.01, or they increase by about 1%, assuming all other variables are held constant. However, remember from the earlier analysis that this effect was not statistically significant.

2.  Based on the likelihood ratio tests that we have conducted, we found that the inclusion of Pressure in the model does not significantly improve the model's fit when Temp is already in the model. This is indicated by a non-significant p-value (p = 0.2156) when we compared the model with Temp and Pressure to the model with Temp alone.

On the contrary, when we compared the model with Pressure alone to the model with Temp and Pressure, we observed a significant improvement in the model fit with the inclusion of Temp (p = 0.005359).

Taking these results together, it would be reasonable to exclude Pressure from the model. By doing so, we would have a simpler model that is easier to interpret, without a significant loss in predictive power.

Excluding Pressure from the model does not appear to negatively affect the model's ability to predict O-ring failure. The significant predictor, Temp, remains in the model and its coefficient remains statistically significant, indicating that it is still able to predict O-ring failure effectively.

## Confidence Intervals (20 points)

No matter what you determined about using or dropping `pressure`, for this section begin by considering the simplified model $logit(\pi) = \beta_0 + \beta_1 Temp$, where $\pi$ is the probability of an O-ring failure. Complete the following:

1.  Estimate the logistic regression model.

```{r, result = FALSE}
model_temp <- glm(O.ring ~ Temp, family = binomial(link = "logit"), data = data)
coef(model_temp)
```

2.  Determine if a quadratic term is needed in the model for the temperature in this model.

```{r, result = FALSE}
model_temp_quad <- glm(O.ring ~ Temp + I(Temp^2), family = binomial(link = "logit"), data = data)
anova(model_temp, model_temp_quad, test = "Chisq")
```

Not needed.

3.  Construct two plots:
4.  $\pi$ vs. Temp; and,

```{r, fig.width=4, fig.height=4, echo=FALSE}
temp_range <- seq(31, 81, length.out = 100)

prob_est <- predict(model_temp, newdata = data.frame(Temp = temp_range), type = "response")
conf_int <- predict(model_temp, newdata = data.frame(Temp = temp_range), type = "link", se.fit = TRUE)

alpha = 0.05
CI.pred.upper = conf_int$fit + qnorm(0.95)*conf_int$se.fit
CI.pred.lower = conf_int$fit + qnorm(0.05)*conf_int$se.fit
CI.pi.upper = exp(CI.pred.upper)/(1+exp(CI.pred.upper))
CI.pi.lower = exp(CI.pred.lower)/(1+exp(CI.pred.lower))

plot(temp_range, prob_est, type = "l", xlab = "Temperature", ylab = "Probability of O-ring failure", col='red')
lines(temp_range, CI.pi.upper, lty = 2, col='blue')
lines(temp_range, CI.pi.lower, lty = 2, col='blue')
legend("bottomright", 
       legend = c("Confidence Interval", "Fitted Model"),  
       col = c("blue", "red"), 
       pch = c(20, NA),  
       lty = c(NA, 1),  
       cex = 0.8,  
       bty = "n", 
       lwd = c(NA, 2))

```

5.  Expected number of failures vs. Temp.

```{r}
expected_failures <- prob_est * 6 # Assuming there are 6 O-rings
```

```{r, fig.width=4, fig.height=3, echo=FALSE}
expected_failures <- prob_est * 6 # Assuming there are 6 O-rings
plot(temp_range, expected_failures, type = "l", xlab = "Temperature", ylab = "Expected number of O-ring failures")
```

-   Include the 95% Wald confidence interval bands for $\pi$ on the plot. Describe, in your analysis of these plots, why the bands much wider for lower temperatures than for higher temperatures?

3. 
```{r, fig.width=4, ig.height=4, echo=FALSE}
prob_31 <- predict(model_temp, newdata = data.frame(Temp = 31), type = "response")
conf_int_31 <- predict(model_temp, newdata = data.frame(Temp = 31), type = "link", se.fit = TRUE)
CI_31.pred.upper = conf_int_31$fit + qnorm(0.95)*conf_int_31$se.fit
CI_31.pred.lower = conf_int_31$fit + qnorm(0.05)*conf_int_31$se.fit
CI_31.pi.upper = exp(CI_31.pred.upper)/(1+exp(CI_31.pred.upper))
CI_31.pi.lower = exp(CI_31.pred.lower)/(1+exp(CI_31.pred.lower))
```

Based on this model we estimate probability of failure at `r prob_31` with confidence interval from `r CI_31.pi.upper` to `r CI_31.pi.lower`. Data should follow IID principle and be normally distributed to apply this inference procedures.

## Bootstrap Confidence Intervals (30 points)

Rather than relying on asymptotic properties, consider using a parametric bootstrap, as did Dalal, Fowlkes and Hoadley.
To produce a confidence interval, the authors used the 0.05 and 0.95 observed quantiles from the simulated distribution as their 90% confidence interval limits.

Using the parametric bootstrap, compute 90% confidence intervals separately at each integer temperature between 10° and 100° Fahrenheit.

In this section, you should describe your process, justify such a process, and present your results in a way that is compelling for your reader.

```{r, warning = FALSE,fig.width=4,fig.height=4}
set.seed(123) # Set seed for reproducibility
# Fit the initial model to the data
fit <- glm(O.ring ~ Temp, data = data, family = binomial)
# Extract the fitted parameters
alpha <- coef(fit)[1]
beta <- coef(fit)[2]
R <- 1000 # Number of bootstrap resamples
# Preallocate a matrix to store the bootstrap coefficients
bootstrap_coefs <- matrix(NA, nrow = R, ncol = 2,
                          dimnames = list(1:R, names(coef(fit))))
i <- 0 # Run the bootstrap
while (i < R) {
  # Resample dataset, assuming failure is distributed per fitted model
  y_star <- rbinom(n = nrow(data), size = 1, prob = plogis(alpha + beta * data$Temp))
  
  # Fit new model to this data
  fit_star <- glm(y_star ~ data$Temp, family = binomial)
  
  if (fit_star$deviance >0.1) {          # Filter out completely separated cases
  i = i+1
  bootstrap_coefs[i,] <- coef(fit_star)}} # Record the estimated parameters

# Compute the 90% CI for intercept
alpha_CI <- quantile(bootstrap_coefs[, "(Intercept)"], probs = c(0.05, 0.95))

# Preallocate a matrix for the beta confidence intervals
beta_CI <- matrix(NA, nrow = 91, ncol = 2, 
                  dimnames = list(10:100, c("Lower", "Upper")))

# Compute the 90% CI for slope at each temperature
for (temp in 10:100) {
  # Calculate the beta values at this temperature
  beta_at_temp <- alpha_CI + bootstrap_coefs[, "Temp"] * temp
  # Compute the confidence interval
  beta_CI[temp - 9,] <- quantile(beta_at_temp, probs = c(0.05, 0.95))
}

beta_CI_prob <- plogis(beta_CI)
```

## Alternative Specification (10 points)

With the same set of explanatory variables in your final model, estimate a linear regression model. Explain the model results; conduct model diagnostic; and assess the validity of the model assumptions. Would you use the linear regression model or binary logistic regression in this case? Explain why.

```{r, echo= FALSE}
linear_model <- lm(as.numeric(O.ring) ~ Temp + Pressure, data = data)
```
```{r}
linear_model$coefficients
```

```{r, fig.width=4, fig.height=4, echo = FALSE}
par(mfrow=c(2,2))

plot(linear_model$fitted.values, linear_model$residuals)
qqnorm(linear_model$residuals)
qqline(linear_model$residuals)
```

Using a linear model with solely the Temperature variable, we observe that the temperature coefficient is -0.04754, indicating that for every increase of 1 unit in temperature the expected number of O-ring failures decreases by approximately 0.04754, if all other variables are held constant. This effect is statistically significant (p = 0.01268).

When plotting the residuals vs the fitted values we noticed a linear nature however a deviation from the norm existed at the tail. The Q-Q similarly shows a linear pattern that deviates at the tail of the distribution. Due to this deviance from the mean at the tails we believe that the linear model assumptions are not confidently met.

In this case, since the dependent variable is binary (0 or 1), it is not be appropriate to use a linear regression model. Instead, logistic regression, which predicts the log-odds of the outcome, might be more suitable.

# Conclusions (10 points)

Our preferred model to understand O-ring failure in the Challenger disaster was to use temperature. The coefficient of temperature is -0.2321627. This value represents the expected change in log-odds of the event (O-ring failure) for a one unit increase in temperature. In this case, a one degree Fahrenheit increase in temperature is associated with a decrease in the log-odds of the event (O-ring failure) by about 0.2321627. To interpret this in terms of odds, we take the exponential of the coefficient. So the odds multiplier per degree increase in temperature is exp(-0.2321627) = 0.7930665. This means that for each additional degree Fahrenheit, we expect the odds of an O-ring failure to be about 79.31% of the odds at the previous degree. In other words, the odds of failure decrease by about 20.69% for each degree increase in temperature.

In conclusion, based on this model, temperature is a significant predictor of O-ring failure, with the risk of failure decreasing as temperature increases.
