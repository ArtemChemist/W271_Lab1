---
title: "W271 Group Lab 1"
subtitle: "Investigating the 1986 Space Shuttle Challenger Accident"
author: "Please fill in with your names."
output: bookdown::pdf_document2
fontsize: 11pt
geometry: margin=1in
---

\newpagecool

```{=tex}
\begin{abstract} 
This report will, indeed, be abstract. No, instead, describe your goals your approach, and what you learn.
\end{abstract}

```
# Introduction

## Research question
```{r load packages, message=FALSE}
library(car)
library(sandwich)
library(lmtest)
library(knitr)
library(Hmisc)
library(gridExtra)
library(stargazer)
library(mcprofile)
library(readr)
library(dplyr)
library(finalfit)
```

# Data (20 points)

**Complete the following task. In your final submission, please remove this question prompt so that your report reads as a report. The Data Section of this report is worth 20 points.**

-   Conduct a thorough EDA of the data set.

    -   This should include both graphical and tabular analysis as taught in this course.
    -   Since the report has a page-limit, you will have to be selective when choosing visuals to illustrate your key points, associated with a concise explanation of the visuals.

-   This EDA should begin with an inspection of the given dataset; examination of anomalies, missing values, potential of top and/or bottom code etc.

```{r}
data <- read.csv("../data/raw/challenger.csv")
data
```


```{r}
summary(data)
```

```{r}
describe(data)
```

```{r}
# Histograms

hist(data$Temp, main = "Joint Temperature Distribution", xlab = "Temp")
hist(data$Pressure, main = "Field Leak-Pressure Distribution", xlab = "Pressure")
hist(data$O.ring, main = "O-ring Issue Distribution", xlab = "O-ring issue")
```

```{r}
# Define colors based on O.ring values
color <- ifelse(data$O.ring > 0, "red", "dark green")

# Create the scatter plot
plot(data$Temp, data$O.ring, xlab = "Temperature (F)", ylab = "O-ring", main = "O-ring Incidents vs. Temperature", pch = 20, col = color)
# Create a legend
legend("topright", legend = c("Flights with no incidents", "Flight with Incidents"), col = c("dark green", "red"), pch = 20)
```

```{r}
data %>%
  count(O.ring) %>%
  mutate(prop = round(prop.table(n), 2)) %>%
  kable(col.names = c("Incidents", "Number", "Proportion"))
```

```{r}
data <- data %>%
  mutate(failure = if_else(data$O.ring == 1 | data$O.ring == 2, "Yes", "No"))


p5 <- data %>%
  ggplot(aes(failure, Temp)) +
  geom_boxplot(aes(fill = failure)) +
  coord_flip() +
  ggtitle("Figure 1: O-ring Incident by Temperature") +
  theme(plot.title = element_text(lineheight = 1, face = "bold")) +
  ylab("Failure") +
  xlab("Temperature")

p6 <- data %>%
  ggplot(aes(failure, Pressure)) +
  geom_boxplot(aes(fill = failure)) +
  coord_flip() +
  ggtitle("Figure 1: O-ring Incident by Pressure") +
  theme(plot.title = element_text(lineheight = 1, face = "bold")) +
  ylab("Failure") +
  xlab("Pressure")

plot_box <- function(data, x, y, title) {
  ggplot(data, aes(factor(x), y)) +
    geom_boxplot(aes(fill = factor(x))) +
    geom_jitter() +
    coord_flip() +
    ggtitle(title) +
    theme(plot.title = element_text(lineheight = 1, face = "bold"))
}

# Admit and GRE
p3 <- plot_box(data,
  x = data$O.ring, y = data$Temp,
  title = "Figure 1: O-ring Incident by Temperature"
)

# Admit and GPA
p4 <- plot_box(data,
  x = data$O.ring, y = data$Pressure,
  title = "Figure 2: O-ring Incident by Pressure"
)

grid.arrange(p5, p6, p3, p4, nrow = 4, ncol = 1)
```
From Data EDA, we don't see any anomalous observations. The launch temperature is approximately normal distributed with a mean of 65-70F. In the sample, approximately 70% of the launches had zero O-ring incidents. Preliminary graphs show that temperature has an effect on O-ring incidents. Lower temperature might increase the probability of O-ring incidents.

## Description

**Complete the following task. In your final submission, please remove this question prompt so that your report reads as a report.**
 
-   Describe the data that you are using. How is this data generated, what is the sampling process that brought it to your availability. If it is helpful, you might describe the population (i.e. the Random Variables) that exist and how samples are produced from these random variables.

-   The authors use logistic regression to estimate the probability an O-ring will fail. In order to use this model, the authors needed to assume that each O-ring is independent for each launch. Discuss why this assumption is necessary and the potential problems with it. Note that a subsequent analysis helped to alleviate the authors' concerns about independence.


The data used in this analysis pertains to the Challenger Space Shuttle disaster and was generated by recording observations of major parameters such as temperature, pressure and O-ring incidents across different shuttle launches. It is sampled from a population of all shuttle launches. The random variables are temperature, pressure, and O-ring failure status. The specific values of these random variables are produced from the actual launches. The sampling process is not a random sampling process because it doesn't involve randomly selecting elements from a larger population. Rather, it involves recording observations from a sequence of events (the shuttle launches) that actually took place. Each launch can be thought of as an experiment where the temperature and pressure are somewhat uncontrollable variables and the O-ring failure status is the outcome.

The assumption of independence in the context of logistic regression means that we assume the outcome (O-ring failure) of one launch doesn't affect the outcome of another launch. This is necessary because if the outcomes were dependent, then conditional probability would need to be accounted for. The dependence of one outcome on another would violate the independence assumption of the logistic regression model. As a result, the estimation of the parameters from the usual logistic regression model would not have statistical significance. The potential problem with this assumption in the current context is that the launches are not truly independent events. Factors like technological advancements, changes in maintenance practices, and learning from past failures could cause later launches to be dependent on earlier ones. Due to technological advancement, more recent launches might see fewer O-ring incidents, skewing the proportion of incidents lower. On the other hand, the physical conditions of the O-rings might not be independent, for example, if the same set of O-rings were used in multiple launches. The metal fatigue as a result of repeated usage of the same set of O-rings could skew the incident proportion higher.

## Key Features

Data Source and Context: The data set is derived from the Challenger Space Shuttle missions, detailing the conditions of each launch, particularly focusing on temperature, pressure, and the crucial aspect of O-ring failure. The observations represent specific shuttle launches under various conditions.

Variables: The primary variables in this data set include temperature (Temp), pressure (Pressure), and the status of the O-ring (O.ring). Each variable provides critical information about the conditions during each space shuttle launch.

Target Variable: The target variable in the analysis is O-ring failure, a binary variable that indicates whether an O-ring failure occurred during a launch (1 = failure, 0 = no failure).

Explanatory Variables: The temperature and pressure during each launch are the explanatory variables. These continuous variables provide context for the conditions that may influence O-ring failure.

Data Structure: The data set is structured as a cross-sectional data set, with each row representing a different space shuttle launch. Random variables "temperature" and "pressure" are column variables "Temp" and "Pressure", respectively.

# Analysis

Temperature (Temp):

Distribution: The temperature during the launches varied from 53°F to 81°F. The distribution of temperature values was fairly normal but slightly skewed towards the higher end.

Relationship with O-ring failure: The exploratory data analysis and logistic regression model indicated a significant relationship between temperature and O-ring failure. Specifically, lower temperatures were associated with a higher risk of O-ring failure. This relationship was found to be statistically significant, making temperature a key variable to consider when predicting O-ring failure.

Insights: The fact that colder temperatures increase the risk of O-ring failure can have important practical implications. For example, it could inform decisions about the optimal conditions for launching a space shuttle to minimize the risk of O-ring failure.

Pressure (Pressure):

Distribution: The pressure during the launches was mostly around 200 psi, with a few instances of lower pressure at 50 psi and 100 psi. The distribution was therefore skewed towards the higher end.

Relationship with O-ring failure: The exploratory data analysis and logistic regression model did not indicate a statistically significant relationship between pressure and O-ring failure. Although pressure was initially included as an explanatory variable in the model, the likelihood ratio test suggested that it did not significantly improve the model's fit to the data.

Insights: While it might seem logical that higher pressure could increase the risk of O-ring failure, the data did not provide statistical evidence for this. Therefore, based on this data set, pressure does not appear to be a key variable to consider when predicting O-ring failure.

In conclusion, while both temperature and pressure are important factors in the overall operating conditions during a space shuttle launch, temperature appears to be the more crucial variable when it comes to predicting O-ring failure.


## Reproducing Previous Analysis (10 points)

**Your analysis should address the following two questions. In your final submission, please remove this question prompt so that your report reads as a report.**

1.  Estimate the logistic regression model that the authors present in their report -- include the variables as linear terms in the model. Evaluate, using likelihood ratio tests, the statistical significance of each explanatory variable in the model. Evaluate, using the context and data understanding that you have created in the **Data** section of this report, the practical significance of each explanatory variable in the model.


```{r}
# Check if O.ring is a factor, if not convert it
if (!is.factor(data$O.ring)) {
  data$O.ring <- as.factor(data$O.ring)
}

# Fit the logistic regression model
model <- glm(O.ring ~ Temp + Pressure, data = data, family = binomial(link = "logit"))

# Fit reduced models
model_temp <- glm(O.ring ~ Temp, data = data, family = binomial(link = "logit"))
model_pressure <- glm(O.ring ~ Pressure, data = data, family = binomial(link = "logit"))

summary(model)
```
The coefficient of -0.228671 indicates that for each additional degree in temperature, the log-odds of O-ring failure decrease by about 0.23, holding pressure constant. This would suggest that higher temperatures are associated with a lower risk of O-ring failure. Because the p-value (0.0376) is below 0.05, this result is statistically significant at the 5% level.

The coefficient of 0.010400 suggests that for each additional unit increase in pressure, the log-odds of O-ring failure increase by about 0.01, holding temperature constant. However, the p-value (0.2468) is greater than 0.05, suggesting that this result is not statistically significant at the 5% level. Therefore, based on this model, pressure does not have a significant effect on the log-odds of O-ring failure.

From a practical perspective, temperature seems to have a significant effect on O-ring failure, with higher temperatures reducing the likelihood of failure. However, pressure doesn't seem to significantly affect the likelihood of failure.

```{r}
# Perform likelihood ratio tests
anova(model_temp, model, test = "LRT")
```
The test compares the deviances of the two models to assess whether adding Pressure to the model significantly improves the model fit. The deviance difference (1.5331) is not statistically significant (p=0.2156), as indicated by the Pr(>Chi) value. This means that, based on this test, adding Pressure as a predictor does not significantly improve the model.

```{r}
anova(model_pressure, model, test = "LRT")
```
The test compares the deviances of the two models to assess whether adding Temp to the model significantly improves the model fit. The deviance difference (7.7542) is statistically significant (p=0.005359), as indicated by the Pr(>Chi) value. This means that adding Temp as a predictor significantly improves the model.

In summary, these results suggest that Temp is a significant predictor of O.ring failure, while Pressure is not, at least not when added to a model that already includes Temp.


```{r}
exp(coef(model))
```

The exponentiated coefficient for Temp is 0.7955903. This means that for each unit increase in Temp, the odds of O-ring failure are multiplied by approximately 0.80, or in other words, they decrease by about 20%, assuming all other variables are held constant. This matches with our previous interpretation that an increase in temperature decreases the likelihood of O-ring failure.

The exponentiated coefficient for Pressure is 1.010454. This means that for each unit increase in Pressure, the odds of O-ring failure are multiplied by approximately 1.01, or they increase by about 1%, assuming all other variables are held constant. However, remember from the earlier analysis that this effect was not statistically significant.


2.  Dalal, Fowlkes, and Hoadley (1989) chose to remove `pressure` from the model based on their likelihood ratio tests. Critically evaluate, using your test results and understanding of the question and data, whether `pressure` should be included in the model, or instead, `pressure` should not be included in the model. Your report needs to make a determination, argue why it is most appropriate choice, and make note of how (if at all) the model results are affected by the choice of including or excluding `pressure`.

Based on the likelihood ratio tests that we have conducted, we found that the inclusion of Pressure in the model does not significantly improve the model's fit when Temp is already in the model. This is indicated by a non-significant p-value (p = 0.2156) when we compared the model with Temp and Pressure to the model with Temp alone.

On the contrary, when we compared the model with Pressure alone to the model with Temp and Pressure, we observed a significant improvement in the model fit with the inclusion of Temp (p = 0.005359).

Taking these results together, it would be reasonable to exclude Pressure from the model. By doing so, we would have a simpler model that is easier to interpret, without a significant loss in predictive power.

Excluding Pressure from the model does not appear to negatively affect the model's ability to predict O-ring failure. The significant predictor, Temp, remains in the model and its coefficient remains statistically significant, indicating that it is still able to predict O-ring failure effectively.


## Confidence Intervals (20 points)

No matter what you determined about using or dropping `pressure`, for this section begin by considering the simplified model $logit(\pi) = \beta_0 + \beta_1 Temp$, where $\pi$ is the probability of an O-ring failure. Complete the following:

1.  Estimate the logistic regression model.

```{r}
model_temp <- glm(O.ring ~ Temp, family = binomial(link = "logit"), data = data)
coef(model_temp)
```

```{r}
data
```


2.  Determine if a quadratic term is needed in the model for the temperature in this model.

```{r}
model_temp_quad <- glm(O.ring ~ Temp + I(Temp^2), family = binomial(link = "logit"), data = data)
anova(model_temp, model_temp_quad, test = "Chisq")
```
Not needed.

3.  Construct two plots:
4.  $\pi$ vs. Temp; and,

```{r}
temp_range <- seq(31, 81, length.out = 100)

prob_est <- predict(model_temp, newdata = data.frame(Temp = temp_range), type = "response")
conf_int <- predict(model_temp, newdata = data.frame(Temp = temp_range), type = "link", se.fit = TRUE)

alpha = 0.05
CI.pred.upper = conf_int$fit + qnorm(0.95)*conf_int$se.fit
CI.pred.lower = conf_int$fit + qnorm(0.05)*conf_int$se.fit
CI.pi.upper = exp(CI.pred.upper)/(1+exp(CI.pred.upper))
CI.pi.lower = exp(CI.pred.lower)/(1+exp(CI.pred.lower))

plot(temp_range, prob_est, type = "l", xlab = "Temperature", ylab = "Probability of O-ring failure", col='red')
lines(temp_range, CI.pi.upper, lty = 2, col='blue')
lines(temp_range, CI.pi.lower, lty = 2, col='blue')
legend("bottomright", 
       legend = c("Confidence Interval", "Fitted Model"),  
       col = c("blue", "red"), 
       pch = c(20, NA),  
       lty = c(NA, 1),  
       cex = 0.8,  
       bty = "n", 
       lwd = c(NA, 2))
```

5.  Expected number of failures vs. Temp.

```{r}
expected_failures <- prob_est * 6 # Assuming there are 6 O-rings
plot(temp_range, expected_failures, type = "l", xlab = "Temperature", ylab = "Expected number of O-ring failures")
```

Specific requirements for these plots:

-   Use a temperature range of 31° to 81° on the x-axis even though the minimum temperature in the data set was 53°.\

-   Include the 95% Wald confidence interval bands for $\pi$ on the plot. Describe, in your analysis of these plots, why the bands much wider for lower temperatures than for higher temperatures?

3.  The temperature was 31° at launch for the Challenger in 1986. Estimate the probability of an O-ring failure using this temperature, and compute a corresponding confidence interval. Discuss what assumptions need to be made in order to apply the inference procedures.


```{r}

prob_31 <- predict(model_temp, newdata = data.frame(Temp = 31), type = "response")
conf_int_31 <- predict(model_temp, newdata = data.frame(Temp = 31), type = "link", se.fit = TRUE)
CI_31.pred.upper = conf_int_31$fit + qnorm(0.95)*conf_int_31$se.fit
CI_31.pred.lower = conf_int_31$fit + qnorm(0.05)*conf_int_31$se.fit
CI_31.pi.upper = exp(CI_31.pred.upper)/(1+exp(CI_31.pred.upper))
CI_31.pi.lower = exp(CI_31.pred.lower)/(1+exp(CI_31.pred.lower))

prob_31
CI_31.pi.upper
CI_31.pi.lower

```

## Bootstrap Confidence Intervals (30 points)

Rather than relying on asymptotic properties, consider using a parametric bootstrap, as did Dalal, Fowlkes and Hoadley. To do this:

1.  Simulate a large number of data sets (n = 23 for each) by re-sampling with replacement from the data.
2.  Estimate a model for each dataset.
3.  Compute the effect at a specific temperature of interest.

To produce a confidence interval, the authors used the 0.05 and 0.95 observed quantiles from the simulated distribution as their 90% confidence interval limits.

Using the parametric bootstrap, compute 90% confidence intervals separately at each integer temperature between 10° and 100° Fahrenheit.

In this section, you should describe your process, justify such a process, and present your results in a way that is compelling for your reader.

```{r load packages for homework 2, message=FALSE}
library(tidyverse)
library(patchwork)
```

```{r read challenger data, message=FALSE}
data$failure <- data$O.ring
data$failure[data$failure > 0] <- 1
data$failure <- factor(data$failure, levels = c(0, 1))

data$Kelvin <- (459.67 + data$Temp) * 5 / 9
```
```{r Add a row}
delta <- data.frame(24, 31, 200, 0, 6, 0, 273)
names(delta) <- names(data)
data <- rbind(data, delta)
```


```{r Bootstrap non-parametric, warning=FALSE}
## Non-parametric bootstrap
intercepts <- vector()
slopes <- vector()
products <- vector()
idx_seq <- seq(1,24)
counter = 0

for (i in 1:1000){
  
  # Generate new data 
  new_data = data[sample(idx_seq, 24, replace = TRUE),]
  
  # Run glm on this new data 
  model_i <- glm(formula = failure ~ Kelvin, family = binomial(), data = new_data)
  
  # If model converged, add coefficients to the vectors
  if (model_i$deviance > 0.1) {
    counter = counter + 1
    intercepts[counter] <- model_i$coefficients[1]
    slopes[counter] <- model_i$coefficients[2]
    products[counter] <- model_i$coefficients[1]*model_i$coefficients[2]
  }
}

intercept_boot <- mean(intercepts) # bootstrap estimate of intercept
intercept_boot

intercept_boot_var <- var(intercepts) # variance of intercept
intercept_boot_var

slope_boot <- mean(slopes)# bootstrap estimate of slope
slope_boot

slope_boot_var <- var(slopes) # variance of slope
slope_boot_var

covar_boot <- mean(products) - slope_boot*intercept_boot # bootstrap estimate of covariance
covar_boot

bootstrap_df <- cbind.data.frame(intercepts, slopes)
```
```{r graph}
library (ggpubr)

inter_polt <- ggplot(data = bootstrap_df) +
  aes(x = intercepts) +
  geom_histogram()

slope_plot <- ggplot(data = bootstrap_df) +
  aes(x = slopes) +
  geom_histogram()

ggarrange(inter_polt, slope_plot,
          labels = c("Intercepts", "Slopes"),
          ncol = 2, nrow = 1)
```

```{r Inference Bootstrap non-param}
temps <- seq(270, 320)
temps_f <- seq(270, 320)*1.8 - 459

logit <- intercept_boot + temps*slope_boot
odds <- exp(logit)
boot_pi <- odds/(1+odds)

logit_lower <- logit - 1.96*sqrt(intercept_boot_var + 2* covar_boot * temps + slope_boot_var*temps^2  )
odds_lower <- exp(logit_lower)
boot_pi_lower <- odds_lower/(1+odds_lower)

logit_upper <- logit + 1.96*sqrt(intercept_boot_var + 2* covar_boot * temps + slope_boot_var*temps^2  )
odds_upper <- exp(logit_upper)
boot_pi_upper <- odds_upper/(1+odds_upper)

infer_df = cbind.data.frame(temps_f, boot_pi, boot_pi_lower, boot_pi_upper)

ggplot(data = infer_df) +
  aes(x = temps_f, y = boot_pi) +
  geom_line()
```



``` {r Parametric bootstrap}

## Parametric bootstrap
intercepts_param <- vector()
slopes_param <- vector()
products_param <- vector()
for (i in 1:1000){ 
    
    intercepts_param[i] <- rnorm(1, mean = intercept_boot, sd = sd(intercepts))
    slopes_param[i] <- rnorm(1, mean = slope_boot, sd = sd(slopes))
    products_param[i] <- slopes_param[i] * intercepts_param[i]

}

intercept_param <- mean(intercepts_param) # bootstrap estimate of intercept
intercept_param

intercept_param_var <- var(intercepts_param) # standard errors of intercept
intercept_param_var

slope_param <- mean(slopes_param)# bootstrap estimate of slope
slope_param

slope_param_var <- var(slopes_param) # standard errors of slope
slope_param_var

covar_param <- mean(products_param) - slope_param*intercept_param # bootstrap estimate of covariance
covar_param

bootstrap_param_df <- cbind.data.frame(intercept_param, slope_param)
```

```{r Inference Bootstrap param}
temps <- seq(270, 320)
temps_f <- seq(270, 320)*1.8 - 459

logit <- intercept_param + temps*slope_param
odds <- exp(logit)
boot_pi <- odds/(1+odds)

logit_lower <- logit - 1.96*sqrt(intercept_param_var + 2* covar_param * temps +slope_param_var*temps^2  )
odds_lower <- exp(logit_lower)
boot_pi_lower <- odds_lower/(1+odds_lower)

logit_upper <- logit + 1.96*sqrt(intercept_boot_var + 2* covar_param * temps + slope_param_var*temps^2  )
odds_upper <- exp(logit_upper)
boot_pi_upper <- odds_upper/(1+odds_upper)

infer_df_param = cbind.data.frame(temps_f, boot_pi, boot_pi_lower, boot_pi_upper)

ggplot(data = infer_df_param) +
  aes(x = temps_f, y = boot_pi) +
  geom_line()
```

```{r Inference Modeled}
temps <- seq(270, 320)
modeled_pi <- vector()
counter = 0

for (temp in 270: 320){
  counter = counter+1
  Kelvin <- temp
  print(Kelvin)
  modeled_pi[counter] <- predict(model_zero, newdata = data.frame(Kelvin), type = "response")
  print(modeled_pi[counter])
}

infer_df_model = cbind.data.frame(temps, modeled_pi)

ggplot(data = infer_df_model) +
  aes(x = temps, y = modeled_pi) +
  geom_line()
```



## Alternative Specification (10 points)

With the same set of explanatory variables in your final model, estimate a linear regression model. Explain the model results; conduct model diagnostic; and assess the validity of the model assumptions. Would you use the linear regression model or binary logistic regression in this case? Explain why.

```{r}
linear_model <- lm(O.ring ~ Temp + Pressure, data = data)
summary(linear_model)
```

```{r}
plot(linear_model$fitted.values, linear_model$residuals)
```
```{r}
qqnorm(linear_model$residuals)
qqline(linear_model$residuals)
```
In this case, since the dependent variable is binary (0 or 1), it might not be appropriate to use a linear regression model. Linear regression may predict values outside the range [0,1] which doesn't make sense for a binary outcome. Instead, logistic regression, which predicts the log-odds of the outcome, might be more suitable.

# Conclusions (10 points)

Interpret the main result of your preferred model in terms of both odds and probability of failure. Summarize this result with respect to the question(s) being asked and key takeaways from the analysis.
