---
title: "W271 Group Lab 1"
subtitle: "Investigating the 1986 Space Shuttle Challenger Accident"
author: "Please fill in with your names."
output: bookdown::pdf_document2
fontsize: 11pt
geometry: margin=1in
---

\newpage

```{=tex}
\begin{abstract} 
This report will, indeed, be abstract. No, instead, describe your goals your approach, and what you learn.
\end{abstract}
```
# Introduction

## Research question

# Data (20 points)

**Complete the following task. In your final submission, please remove this question prompt so that your report reads as a report. The Data Section of this report is worth 20 points.**

-   Conduct a thorough EDA of the data set.

    -   This should include both graphical and tabular analysis as taught in this course.
    -   Since the report has a page-limit, you will have to be selective when choosing visuals to illustrate your key points, associated with a concise explanation of the visuals.

-   This EDA should begin with an inspection of the given dataset; examination of anomalies, missing values, potential of top and/or bottom code etc.

## Description

**Complete the following task. In your final submission, please remove this question prompt so that your report reads as a report.**

-   Describe the data that you are using. How is this data generated, what is the sampling process that brought it to your availability. If it is helpful, you might describe the population (i.e. the Random Variables) that exist and how samples are produced from these random variables.
-   The authors use logistic regression to estimate the probability an O-ring will fail. In order to use this model, the authors needed to assume that each O-ring is independent for each launch. Discuss why this assumption is necessary and the potential problems with it. Note that a subsequent analysis helped to alleviate the authors' concerns about independence.

## Key Features

(AKA EDA)

# Analysis

## Reproducing Previous Analysis (10 points)

**Your analysis should address the following two questions. In your final submission, please remove this question prompt so that your report reads as a report.**

1.  Estimate the logistic regression model that the authors present in their report -- include the variables as linear terms in the model. Evaluate, using likelihood ratio tests, the statistical significance of each explanatory variable in the model. Evaluate, using the context and data understanding that you have created in the **Data** section of this report, the practical significance of each explanatory variable in the model.

2.  Dalal, Fowlkes, and Hoadley (1989) chose to remove `pressure` from the model based on their likelihood ratio tests. Critically evaluate, using your test results and understanding of the question and data, whether `pressure` should be included in the model, or instead, `pressure` should not be included in the model. Your report needs to make a determination, argue why it is most appropriate choice, and make note of how (if at all) the model results are affected by the choice of including or excluding `pressure`.

## Confidence Intervals (20 points)

No matter what you determined about using or dropping `pressure`, for this section begin by considering the simplified model $logit(\pi) = \beta_0 + \beta_1 Temp$, where $\pi$ is the probability of an O-ring failure. Complete the following:

1.  Estimate the logistic regression model.
2.  Determine if a quadratic term is needed in the model for the temperature in this model.
3.  Construct two plots:
4.  $\pi$ vs. Temp; and,
5.  Expected number of failures vs. Temp.

Specific requirements for these plots:

-   Use a temperature range of 31° to 81° on the x-axis even though the minimum temperature in the data set was 53°.\
-   Include the 95% Wald confidence interval bands for $\pi$ on the plot. Describe, in your analysis of these plots, why the bands much wider for lower temperatures than for higher temperatures?

3.  The temperature was 31° at launch for the Challenger in 1986. Estimate the probability of an O-ring failure using this temperature, and compute a corresponding confidence interval. Discuss what assumptions need to be made in order to apply the inference procedures.

## Bootstrap Confidence Intervals (30 points)

Rather than relying on asymptotic properties, consider using a parametric bootstrap, as did Dalal, Fowlkes and Hoadley. To do this:

1.  Simulate a large number of data sets (n = 23 for each) by re-sampling with replacement from the data.
2.  Estimate a model for each dataset.
3.  Compute the effect at a specific temperature of interest.

To produce a confidence interval, the authors used the 0.05 and 0.95 observed quantiles from the simulated distribution as their 90% confidence interval limits.

Using the parametric bootstrap, compute 90% confidence intervals separately at each integer temperature between 10° and 100° Fahrenheit.

In this section, you should describe your process, justify such a process, and present your results in a way that is compelling for your reader.

```{r load packages for homework 2, message=FALSE}
library(tidyverse)
library(patchwork)
```

```{r read challenger data, message=FALSE}
setwd("~/source_code/W271_Lab1")
oring_df <- read_csv("./data/raw/challenger.csv")
oring_df$failure <- oring_df$O.ring
oring_df$failure[oring_df$failure >0] <- 1
oring_df$failure <-factor(oring_df$failure, levels = c(0,1))

oring_df$Kelvin <- (459.67+ oring_df$Temp)*5/9
```
```{r Add a row}
delta <-data.frame(24, 31, 200, 0, 6, 0, 273)
names(delta) <- names(oring_df)
oring_df <- rbind(oring_df, delta)

```

```{r}
model_zero <- glm(formula=failure~Kelvin, family=binomial(), data=oring_df)
summary(model_zero)
```

```{r Bootstrap non-parametric, warning=FALSE}
## Non-parametric bootstrap
intercepts <- vector()
slopes <- vector()
products <- vector()
idx_seq <- seq(1,24)
counter = 0

for (i in 1:1000){
  
  # Generate new data 
  new_data = oring_df[sample(idx_seq, 24, replace = TRUE),]
  
  # Run glm on this new data 
  model_i <- glm(formula = failure ~ Kelvin, family = binomial(), data = new_data)
  
  # If model converged, add coefficients to the vectors
  if (model_i$deviance > 0.1) {
    counter = counter + 1
    intercepts[counter] <- model_i$coefficients[1]
    slopes[counter] <- model_i$coefficients[2]
    products[counter] <- model_i$coefficients[1]*model_i$coefficients[2]
  }
}

intercept_boot <- mean(intercepts) # bootstrap estimate of intercept
intercept_boot

intercept_boot_var <- var(intercepts) # variance of intercept
intercept_boot_var

slope_boot <- mean(slopes)# bootstrap estimate of slope
slope_boot

slope_boot_var <- var(slopes) # variance of slope
slope_boot_var

covar_boot <- mean(products) - slope_boot*intercept_boot # bootstrap estimate of covariance
covar_boot

bootstrap_df <- cbind.data.frame(intercepts, slopes)
```
```{r graph}
library (ggpubr)

inter_polt <- ggplot(data = bootstrap_df) +
  aes(x = intercepts) +
  geom_histogram()

slope_plot <- ggplot(data = bootstrap_df) +
  aes(x = slopes) +
  geom_histogram()

ggarrange(inter_polt, slope_plot,
          labels = c("Intercepts", "Slopes"),
          ncol = 2, nrow = 1)
```

```{r Inference Bootstrap non-param}
temps <- seq(270, 320)
temps_f <- seq(270, 320)*1.8 - 459

logit <- intercept_boot + temps*slope_boot
odds <- exp(logit)
boot_pi <- odds/(1+odds)

logit_lower <- logit - 1.96*sqrt(intercept_boot_var + 2* covar_boot * temps + slope_boot_var*temps^2  )
odds_lower <- exp(logit_lower)
boot_pi_lower <- odds_lower/(1+odds_lower)

logit_upper <- logit + 1.96*sqrt(intercept_boot_var + 2* covar_boot * temps + slope_boot_var*temps^2  )
odds_upper <- exp(logit_upper)
boot_pi_upper <- odds_upper/(1+odds_upper)

infer_df = cbind.data.frame(temps_f, boot_pi, boot_pi_lower, boot_pi_upper)

ggplot(data = infer_df) +
  aes(x = temps_f, y = boot_pi) +
  geom_line()
```



``` {r Parametric bootstrap}

## Parametric bootstrap
intercepts_param <- vector()
slopes_param <- vector()
products_param <- vector()
for (i in 1:1000){ 
    
    intercepts_param[i] <- rnorm(1, mean = intercept_boot, sd = sd(intercepts))
    slopes_param[i] <- rnorm(1, mean = slope_boot, sd = sd(slopes))
    products_param[i] <- slopes_param[i] * intercepts_param[i]

}

intercept_param <- mean(intercepts_param) # bootstrap estimate of intercept
intercept_param

intercept_param_var <- var(intercepts_param) # standard errors of intercept
intercept_param_var

slope_param <- mean(slopes_param)# bootstrap estimate of slope
slope_param

slope_param_var <- var(slopes_param) # standard errors of slope
slope_param_var

covar_param <- mean(products_param) - slope_param*intercept_param # bootstrap estimate of covariance
covar_param

bootstrap_param_df <- cbind.data.frame(intercept_param, slope_param)
```

```{r Inference Bootstrap param}
temps <- seq(270, 320)
temps_f <- seq(270, 320)*1.8 - 459

logit <- intercept_param + temps*slope_param
odds <- exp(logit)
boot_pi <- odds/(1+odds)

logit_lower <- logit - 1.96*sqrt(intercept_param_var + 2* covar_param * temps +slope_param_var*temps^2  )
odds_lower <- exp(logit_lower)
boot_pi_lower <- odds_lower/(1+odds_lower)

logit_upper <- logit + 1.96*sqrt(intercept_boot_var + 2* covar_param * temps + slope_param_var*temps^2  )
odds_upper <- exp(logit_upper)
boot_pi_upper <- odds_upper/(1+odds_upper)

infer_df_param = cbind.data.frame(temps_f, boot_pi, boot_pi_lower, boot_pi_upper)

ggplot(data = infer_df_param) +
  aes(x = temps_f, y = boot_pi) +
  geom_line()
```

```{r Inference Modeled}
temps <- seq(270, 320)
modeled_pi <- vector()
counter = 0

for (temp in 270: 320){
  counter = counter+1
  Kelvin <- temp
  print(Kelvin)
  modeled_pi[counter] <- predict(model_zero, newdata = data.frame(Kelvin), type = "response")
  print(modeled_pi[counter])
}

infer_df_model = cbind.data.frame(temps, modeled_pi)

ggplot(data = infer_df_model) +
  aes(x = temps, y = modeled_pi) +
  geom_line()
```



## Alternative Specification (10 points)

With the same set of explanatory variables in your final model, estimate a linear regression model. Explain the model results; conduct model diagnostic; and assess the validity of the model assumptions. Would you use the linear regression model or binary logistic regression in this case? Explain why.

# Conclusions (10 points)

Interpret the main result of your preferred model in terms of both odds and probability of failure. Summarize this result with respect to the question(s) being asked and key takeaways from the analysis.
